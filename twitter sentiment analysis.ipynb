{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way. \n\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include *Positive, Neutral*, and *Negative*, *Review Ratings* and *Happy, Sad*. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject. \n![Sentiment Analysis](https://media-exp1.licdn.com/dms/image/C4D12AQHPAZFZZxBtng/article-cover_image-shrink_600_2000/0?e=1593648000&v=beta&t=eQAR5WOihE2_ZCCAJbsgNyJlaI_GW7u8lDw45zGbfuU)\n> Sentiment Classification is a perfect problem in NLP for getting started in it. You can really learn a lot of concepts and techniques to master through doing project. Kaggle is a great place to learn and contribute your own ideas and creations. I learnt lot of things from other, now it's my turn to make document my project.\n\nI will go through all the key and fundament concepts of NLP and Sequence Models, which you will learn in this notebook. \n![Sentiment Analysis](https://fiverr-res.cloudinary.com/images/t_main1,q_auto,f_auto,q_auto,f_auto/gigs/121192228/original/677c209a0a064cb9253973d3663684acf91dab84/do-nlp-projects-with-python-nltk-gensim.jpg)\nLet's get started with code without furthur ado.\n\n<font color='red'> If you find this notebook helpful, please leave a UPVOTE to encourage me</font>","metadata":{}},{"cell_type":"markdown","source":"##  Importing Dependencies\n   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow Version\",tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-05-06T16:18:04.494052Z","iopub.execute_input":"2023-05-06T16:18:04.494436Z","iopub.status.idle":"2023-05-06T16:18:05.522466Z","shell.execute_reply.started":"2023-05-06T16:18:04.494371Z","shell.execute_reply":"2023-05-06T16:18:05.521326Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nTensorflow Version 2.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  Dataset Preprocessing\nIn this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n                 encoding = 'latin',header=None)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:05.524691Z","iopub.execute_input":"2023-05-06T16:18:05.525320Z","iopub.status.idle":"2023-05-06T16:18:11.761981Z","shell.execute_reply.started":"2023-05-06T16:18:05.525015Z","shell.execute_reply":"2023-05-06T16:18:11.760889Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   0           1                             2         3                4  \\\n0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n\n                                                   5  \n0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1  is upset that he can't update his Facebook by ...  \n2  @Kenichan I dived many times for the ball. Man...  \n3    my whole body feels itchy and like its on fire   \n4  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"You can see the columns are without any proper names. Lets rename them for our reference","metadata":{}},{"cell_type":"code","source":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.763258Z","iopub.execute_input":"2023-05-06T16:18:11.763603Z","iopub.status.idle":"2023-05-06T16:18:11.786787Z","shell.execute_reply.started":"2023-05-06T16:18:11.763551Z","shell.execute_reply":"2023-05-06T16:18:11.785274Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   sentiment          id                          date     query  \\\n0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n           user_id                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  \n3          ElleCTF    my whole body feels itchy and like its on fire   \n4           Karoli  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>user_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We are going to train only on text to classify its sentiment. So we can ditch the rest of the useless columns.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.789615Z","iopub.execute_input":"2023-05-06T16:18:11.791766Z","iopub.status.idle":"2023-05-06T16:18:11.830873Z","shell.execute_reply.started":"2023-05-06T16:18:11.791716Z","shell.execute_reply":"2023-05-06T16:18:11.830102Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.834279Z","iopub.execute_input":"2023-05-06T16:18:11.834779Z","iopub.status.idle":"2023-05-06T16:18:12.296848Z","shell.execute_reply.started":"2023-05-06T16:18:11.834615Z","shell.execute_reply":"2023-05-06T16:18:12.296123Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  sentiment                                               text\n0  Negative  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1  Negative  is upset that he can't update his Facebook by ...\n2  Negative  @Kenichan I dived many times for the ball. Man...\n3  Negative    my whole body feels itchy and like its on fire \n4  Negative  @nationwideclass no, it's not behaving at all....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Negative</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Negative</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Negative</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Negative</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Negative</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes.","metadata":{}},{"cell_type":"code","source":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.299816Z","iopub.execute_input":"2023-05-06T16:18:12.300213Z","iopub.status.idle":"2023-05-06T16:18:12.525122Z","shell.execute_reply.started":"2023-05-06T16:18:12.300153Z","shell.execute_reply":"2023-05-06T16:18:12.524019Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Sentiment Data Distribution')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"It's a very good dataset without any skewness. Thank Goodness.\n\nNow let us explore the data we having here... ","metadata":{}},{"cell_type":"code","source":"import random\nrandom_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\ndf.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.526741Z","iopub.execute_input":"2023-05-06T16:18:12.527306Z","iopub.status.idle":"2023-05-06T16:18:12.665890Z","shell.execute_reply.started":"2023-05-06T16:18:12.527108Z","shell.execute_reply":"2023-05-06T16:18:12.665171Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        sentiment                                               text\n1503887  Positive  Fun and exciting day. Watched He's Just Not Th...\n467559   Negative                         India out of T20 word cup \n862980   Positive  @yarnaboutyarn Yes I am very thankful you to a...\n784550   Negative  @bjornyeo yeah  it's way more fun being &quot;...\n541920   Negative  is Condom shopping at CVS. *sigh* I haven't ha...\n743335   Negative  being an absolute sloth/not celebrating father...\n922530   Positive  yessssssssss  finally have some more call the ...\n306397   Negative  @BiggTim Oh damn  That's so awful. Those poor ...\n503091   Negative  @Xm4n  I'd watch him with you if I'd paid Dire...\n1169403  Positive  @MissYesterday Mars is coming sweet friend  st...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1503887</th>\n      <td>Positive</td>\n      <td>Fun and exciting day. Watched He's Just Not Th...</td>\n    </tr>\n    <tr>\n      <th>467559</th>\n      <td>Negative</td>\n      <td>India out of T20 word cup</td>\n    </tr>\n    <tr>\n      <th>862980</th>\n      <td>Positive</td>\n      <td>@yarnaboutyarn Yes I am very thankful you to a...</td>\n    </tr>\n    <tr>\n      <th>784550</th>\n      <td>Negative</td>\n      <td>@bjornyeo yeah  it's way more fun being &amp;quot;...</td>\n    </tr>\n    <tr>\n      <th>541920</th>\n      <td>Negative</td>\n      <td>is Condom shopping at CVS. *sigh* I haven't ha...</td>\n    </tr>\n    <tr>\n      <th>743335</th>\n      <td>Negative</td>\n      <td>being an absolute sloth/not celebrating father...</td>\n    </tr>\n    <tr>\n      <th>922530</th>\n      <td>Positive</td>\n      <td>yessssssssss  finally have some more call the ...</td>\n    </tr>\n    <tr>\n      <th>306397</th>\n      <td>Negative</td>\n      <td>@BiggTim Oh damn  That's so awful. Those poor ...</td>\n    </tr>\n    <tr>\n      <th>503091</th>\n      <td>Negative</td>\n      <td>@Xm4n  I'd watch him with you if I'd paid Dire...</td>\n    </tr>\n    <tr>\n      <th>1169403</th>\n      <td>Positive</td>\n      <td>@MissYesterday Mars is coming sweet friend  st...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Looks like we have a nasty data in text. Because in general we use lot of punctuations and other words without any contextual meaning. It have no value as feature to the model we are training. So we need to get rid of them.\n\n# Text Preprocessing\nTweet texts often consists of other user mentions, hyperlink texts, emoticons and punctuations. In order to use them for learning using a Language Model. We cannot permit those texts for training a model. So we have to clean the text data using various preprocessing and cleansing methods. Let's continue\n![Data Science Meme](https://miro.medium.com/max/800/1*Xhm9c9qDfXa3ZCQjiOvm_w.jpeg)\n","metadata":{}},{"cell_type":"markdown","source":"### Stemming/ Lematization\nFor grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes.* Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\nStemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes. \n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n![Stemming and Lematization](https://qph.fs.quoracdn.net/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n\n### Hyperlinks and Mentions\nTwitter is a social media platform where people can tag and mentions other people's ID and share videos and blogs from internet. So the tweets often contain lots of Hyperlinks and twitter mentions.\n\n- Twitter User Mentions - Eg. @arunrk7, @andrewng\n- Hyperlinks - Eg. https://keras.io, https://tensorflow.org\n\n### Stopwords\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some stopwords are...\n![Stopwords English](https://4.bp.blogspot.com/-yiEr-jCVv38/Wmk10d84DYI/AAAAAAAAk0o/IfgjfjpgrxM5NosUQrGw7PtLvgr6DAG8ACLcBGAs/s1600/Screen%2BShot%2B2018-01-24%2Bat%2B5.41.21%2BPM.png)\n\nThat looks like a tedious process, isn't?. Don't worry there is always some library in Python to do almost any work. The world is great!!!\n\n**NLTK** is a python library which got functions to perform text processing task for NLP.\n\n","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.667837Z","iopub.execute_input":"2023-05-06T16:18:12.668148Z","iopub.status.idle":"2023-05-06T16:18:12.676768Z","shell.execute_reply.started":"2023-05-06T16:18:12.668102Z","shell.execute_reply":"2023-05-06T16:18:12.675921Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.680165Z","iopub.execute_input":"2023-05-06T16:18:12.680489Z","iopub.status.idle":"2023-05-06T16:18:12.688068Z","shell.execute_reply.started":"2023-05-06T16:18:12.680442Z","shell.execute_reply":"2023-05-06T16:18:12.686915Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.689527Z","iopub.execute_input":"2023-05-06T16:18:12.689899Z","iopub.status.idle":"2023-05-06T16:19:05.189371Z","shell.execute_reply.started":"2023-05-06T16:18:12.689831Z","shell.execute_reply":"2023-05-06T16:19:05.188360Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test Split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.190663Z","iopub.execute_input":"2023-05-06T16:19:05.191142Z","iopub.status.idle":"2023-05-06T16:19:05.203186Z","shell.execute_reply.started":"2023-05-06T16:19:05.191092Z","shell.execute_reply":"2023-05-06T16:19:05.201126Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.206401Z","iopub.execute_input":"2023-05-06T16:19:05.206909Z","iopub.status.idle":"2023-05-06T16:19:05.498179Z","shell.execute_reply.started":"2023-05-06T16:19:05.206864Z","shell.execute_reply":"2023-05-06T16:19:05.497090Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Train Data size: 1280000\nTest Data size 320000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`train_test_split` will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training.","metadata":{}},{"cell_type":"code","source":"train_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.499448Z","iopub.execute_input":"2023-05-06T16:19:05.500034Z","iopub.status.idle":"2023-05-06T16:19:05.512041Z","shell.execute_reply.started":"2023-05-06T16:19:05.499975Z","shell.execute_reply":"2023-05-06T16:19:05.510779Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"        sentiment                                               text\n23786    Negative                                       need friends\n182699   Negative                          im trying call impossible\n476661   Negative  good pace going 3k 13 min missed 5k turn ended...\n1181490  Positive               u gonna shows ny soon luv see u live\n878773   Positive  hell yea get em tattoos ink free wish parents ...\n130866   Negative  yeah need 2 see ur mom calls back first rememb...\n1235876  Positive                           sounds like cup tea sign\n717314   Negative                               tired want sleep wtf\n969880   Positive                                       amazing wish\n748698   Negative  thank god wkrn abc affiliate nashville back mi...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23786</th>\n      <td>Negative</td>\n      <td>need friends</td>\n    </tr>\n    <tr>\n      <th>182699</th>\n      <td>Negative</td>\n      <td>im trying call impossible</td>\n    </tr>\n    <tr>\n      <th>476661</th>\n      <td>Negative</td>\n      <td>good pace going 3k 13 min missed 5k turn ended...</td>\n    </tr>\n    <tr>\n      <th>1181490</th>\n      <td>Positive</td>\n      <td>u gonna shows ny soon luv see u live</td>\n    </tr>\n    <tr>\n      <th>878773</th>\n      <td>Positive</td>\n      <td>hell yea get em tattoos ink free wish parents ...</td>\n    </tr>\n    <tr>\n      <th>130866</th>\n      <td>Negative</td>\n      <td>yeah need 2 see ur mom calls back first rememb...</td>\n    </tr>\n    <tr>\n      <th>1235876</th>\n      <td>Positive</td>\n      <td>sounds like cup tea sign</td>\n    </tr>\n    <tr>\n      <th>717314</th>\n      <td>Negative</td>\n      <td>tired want sleep wtf</td>\n    </tr>\n    <tr>\n      <th>969880</th>\n      <td>Positive</td>\n      <td>amazing wish</td>\n    </tr>\n    <tr>\n      <th>748698</th>\n      <td>Negative</td>\n      <td>thank god wkrn abc affiliate nashville back mi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenization\nGiven a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called *tokens* , perhaps at the same time throwing away certain characters, such as punctuation. The process is called **Tokenization.**\n![Tokenization](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n\n`tokenizer` create tokens for every word in the data corpus and map them to a index using dictionary.\n\n`word_index` contains the index for each word\n\n`vocab_size` represents the total number of word in the data corpus","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.513596Z","iopub.execute_input":"2023-05-06T16:19:05.514441Z","iopub.status.idle":"2023-05-06T16:19:26.245292Z","shell.execute_reply.started":"2023-05-06T16:19:05.514298Z","shell.execute_reply":"2023-05-06T16:19:26.244353Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"},{"name":"stdout","text":"Vocabulary Size : 290575\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we got a `tokenizer` object, which can be used to covert any word into a Key in dictionary (number).\n\nSince we are going to build a sequence model. We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from `pad_sequence` to do our job. It will make all the sequence in one constant length `MAX_SEQUENCE_LENGTH`.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:26.246623Z","iopub.execute_input":"2023-05-06T16:19:26.247171Z","iopub.status.idle":"2023-05-06T16:19:56.742513Z","shell.execute_reply.started":"2023-05-06T16:19:26.247117Z","shell.execute_reply":"2023-05-06T16:19:56.741673Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Training X Shape: (1280000, 30)\nTesting X Shape: (320000, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:56.743805Z","iopub.execute_input":"2023-05-06T16:19:56.744169Z","iopub.status.idle":"2023-05-06T16:19:56.800349Z","shell.execute_reply.started":"2023-05-06T16:19:56.744118Z","shell.execute_reply":"2023-05-06T16:19:56.799625Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding \nWe are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings.","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:56.801674Z","iopub.execute_input":"2023-05-06T16:19:56.802043Z","iopub.status.idle":"2023-05-06T16:19:57.528108Z","shell.execute_reply.started":"2023-05-06T16:19:56.801994Z","shell.execute_reply":"2023-05-06T16:19:57.527081Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"y_train shape: (1280000, 1)\ny_test shape: (320000, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Word Emdedding\nIn Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it. \n\n**Word Embedding** is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nBasically, it's a feature vector representation of words which are used for other natural language processing applications.\n\nWe could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use **Transfer Learning**. We download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like **GloVe & Word2Vec** gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook.\n\n\nIn this notebook, I use **GloVe Embedding from Stanford AI** which can be found [here](https://nlp.stanford.edu/projects/glove/)","metadata":{}},{"cell_type":"code","source":"#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.529562Z","iopub.execute_input":"2023-05-06T16:19:57.530157Z","iopub.status.idle":"2023-05-06T16:19:57.534357Z","shell.execute_reply.started":"2023-05-06T16:19:57.530098Z","shell.execute_reply":"2023-05-06T16:19:57.533417Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 10\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.535771Z","iopub.execute_input":"2023-05-06T16:19:57.536383Z","iopub.status.idle":"2023-05-06T16:19:57.544239Z","shell.execute_reply.started":"2023-05-06T16:19:57.536323Z","shell.execute_reply":"2023-05-06T16:19:57.542716Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.545535Z","iopub.execute_input":"2023-05-06T16:19:57.545819Z","iopub.status.idle":"2023-05-06T16:20:24.680272Z","shell.execute_reply.started":"2023-05-06T16:19:57.545770Z","shell.execute_reply":"2023-05-06T16:20:24.679291Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:20:24.681615Z","iopub.execute_input":"2023-05-06T16:20:24.682165Z","iopub.status.idle":"2023-05-06T16:20:25.162302Z","shell.execute_reply.started":"2023-05-06T16:20:24.682111Z","shell.execute_reply":"2023-05-06T16:20:25.161325Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:20:25.163737Z","iopub.execute_input":"2023-05-06T16:20:25.164353Z","iopub.status.idle":"2023-05-06T16:20:25.175099Z","shell.execute_reply.started":"2023-05-06T16:20:25.164162Z","shell.execute_reply":"2023-05-06T16:20:25.174236Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Model Training - LSTM\nWe are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n\nAs you can see in the word cloud, the some words are predominantly feature in both Positive and Negative tweets. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use **Sequence Models**.\n\n### Sequence Model\n![Sequence Model](https://miro.medium.com/max/1458/1*SICYykT7ybua1gVJDNlajw.png)\n\nReccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction. \n\nFor model architecture, we use\n\n1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n\n2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors. \n\n3) **LSTM** - Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\n4) **Dense** - Fully Connected Layers for classification\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, Attention, Concatenate\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:21:15.091583Z","iopub.execute_input":"2023-05-06T16:21:15.092131Z","iopub.status.idle":"2023-05-06T16:21:15.097002Z","shell.execute_reply.started":"2023-05-06T16:21:15.091900Z","shell.execute_reply":"2023-05-06T16:21:15.096057Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nx1 = x[0]\nprint(x1.shape)\nx1 = tf.reshape(x1,[-1,3328])\nprint(x1.shape)\nx = Concatenate(axis=-1)([x1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:23:48.264426Z","iopub.execute_input":"2023-05-06T16:23:48.264787Z","iopub.status.idle":"2023-05-06T16:23:48.809582Z","shell.execute_reply.started":"2023-05-06T16:23:48.264727Z","shell.execute_reply":"2023-05-06T16:23:48.808573Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"(None, 26, 128)\n(None, 3328)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.160227Z","iopub.execute_input":"2023-05-06T16:24:20.160963Z","iopub.status.idle":"2023-05-06T16:24:20.183200Z","shell.execute_reply.started":"2023-05-06T16:24:20.160700Z","shell.execute_reply":"2023-05-06T16:24:20.182352Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 30)]         0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 30, 300)      87172500    input_4[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d_3 (SpatialDro (None, 30, 300)      0           embedding[3][0]                  \n__________________________________________________________________________________________________\nconv1d_3 (Conv1D)               (None, 26, 64)       96064       spatial_dropout1d_3[0][0]        \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) [(None, 26, 128), (N 66048       conv1d_3[0][0]                   \n__________________________________________________________________________________________________\ntf_op_layer_Reshape_3 (TensorFl [(None, 3328)]       0           bidirectional_3[0][0]            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 3584)         0           tf_op_layer_Reshape_3[0][0]      \n                                                                 bidirectional_3[0][1]            \n                                                                 bidirectional_3[0][2]            \n                                                                 bidirectional_3[0][3]            \n                                                                 bidirectional_3[0][4]            \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 512)          1835520     concatenate_3[0][0]              \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 512)          0           dense_6[0][0]                    \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n==================================================================================================\nTotal params: 89,433,301\nTrainable params: 2,260,801\nNon-trainable params: 87,172,500\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Optimization Algorithm\nThis notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n\n### Callbacks\nCallbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n\n- **LRScheduler** - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n- **ModelCheckPoint** - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.184959Z","iopub.execute_input":"2023-05-06T16:24:20.186101Z","iopub.status.idle":"2023-05-06T16:24:20.247331Z","shell.execute_reply.started":"2023-05-06T16:24:20.186051Z","shell.execute_reply":"2023-05-06T16:24:20.246024Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Let's start training... It takes a heck of a time if training in CPU, be sure your GPU turned on... May the CUDA Cores be with you....","metadata":{}},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.256917Z","iopub.execute_input":"2023-05-06T16:24:20.259044Z","iopub.status.idle":"2023-05-06T16:24:20.272660Z","shell.execute_reply.started":"2023-05-06T16:24:20.258991Z","shell.execute_reply":"2023-05-06T16:24:20.271565Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Training on GPU...\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.277353Z","iopub.execute_input":"2023-05-06T16:24:20.281193Z","iopub.status.idle":"2023-05-06T16:41:11.028014Z","shell.execute_reply.started":"2023-05-06T16:24:20.281134Z","shell.execute_reply":"2023-05-06T16:41:11.026791Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Train on 1280000 samples, validate on 320000 samples\nEpoch 1/10\n1280000/1280000 [==============================] - 106s 83us/sample - loss: 0.5195 - accuracy: 0.7394 - val_loss: 0.4845 - val_accuracy: 0.7635\nEpoch 2/10\n1280000/1280000 [==============================] - 101s 79us/sample - loss: 0.4873 - accuracy: 0.7625 - val_loss: 0.4711 - val_accuracy: 0.7738\nEpoch 3/10\n1280000/1280000 [==============================] - 101s 79us/sample - loss: 0.4769 - accuracy: 0.7694 - val_loss: 0.4655 - val_accuracy: 0.7764\nEpoch 4/10\n1280000/1280000 [==============================] - 101s 79us/sample - loss: 0.4705 - accuracy: 0.7739 - val_loss: 0.4644 - val_accuracy: 0.7778\nEpoch 5/10\n1280000/1280000 [==============================] - 101s 79us/sample - loss: 0.4660 - accuracy: 0.7763 - val_loss: 0.4620 - val_accuracy: 0.7782\nEpoch 6/10\n1280000/1280000 [==============================] - 101s 79us/sample - loss: 0.4627 - accuracy: 0.7780 - val_loss: 0.4608 - val_accuracy: 0.7796\nEpoch 7/10\n1280000/1280000 [==============================] - 100s 78us/sample - loss: 0.4600 - accuracy: 0.7797 - val_loss: 0.4618 - val_accuracy: 0.7788\nEpoch 8/10\n1280000/1280000 [==============================] - 100s 78us/sample - loss: 0.4579 - accuracy: 0.7813 - val_loss: 0.4599 - val_accuracy: 0.7799\nEpoch 9/10\n1280000/1280000 [==============================] - 100s 78us/sample - loss: 0.4555 - accuracy: 0.7824 - val_loss: 0.4587 - val_accuracy: 0.7815\nEpoch 10/10\n1280000/1280000 [==============================] - 100s 78us/sample - loss: 0.4541 - accuracy: 0.7830 - val_loss: 0.4584 - val_accuracy: 0.7807\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_weights('./checkpoints/my_checkpoint')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:45:06.960005Z","iopub.status.idle":"2023-05-06T16:45:06.960566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(128, 5, activation='relu')(x)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nx1 = x[0]\nx1 = tf.reshape(x1,[-1,3328*2])\nx = Concatenate(axis=-1)([x1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel1 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel1.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T17:25:53.350014Z","iopub.execute_input":"2023-05-06T17:25:53.351003Z","iopub.status.idle":"2023-05-06T17:25:53.857949Z","shell.execute_reply.started":"2023-05-06T17:25:53.350385Z","shell.execute_reply":"2023-05-06T17:25:53.857038Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"history = model1.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T17:25:53.859760Z","iopub.execute_input":"2023-05-06T17:25:53.860129Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train on 1280000 samples, validate on 320000 samples\nEpoch 1/10\n 452608/1280000 [=========>....................] - ETA: 1:13 - loss: 0.5399 - accuracy: 0.7223","output_type":"stream"}]},{"cell_type":"code","source":"model1.save_weights('./checkpoints/my_checkpoint1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nlstm1 = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(x[0])\nx = Concatenate(axis=-1)([lstm1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel2 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel2.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.save_weights('./checkpoints/my_checkpoint2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nlstm1 = LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)(x[0])\nlstm2 = LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_state=True)(lstm1)\nconc = Concatenate(axis=-1)([lstm2[0],lstm2[1],lstm2[2],x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(conc)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel3 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel3.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model3.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.save_weights('./checkpoints/my_checkpoint3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\nNow that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","metadata":{}},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment.","metadata":{}},{"cell_type":"code","source":"import math\n\nsentiment =[\"negative\", \"slightly negative\",\"neutral\",\"slightly positive\",\"positive\"]\n\ndef modified_sentiment(score):\n    return math.floor(score*5)\n\nscores2 = model.predict(x_test[0:100])\ny_pred_1d = [sentiment[modified_sentiment(score)] for score in scores]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix\nConfusion Matrix provide a nice overlook at the model's performance in classification task","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Scores","metadata":{}},{"cell_type":"code","source":"print(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis.","metadata":{}},{"cell_type":"markdown","source":"<h3>Some of the resource and people who help me learn some concepts</h3>\n<font color='#008080'>\n    <ul>\n        <li> <b>Andrew NG's Seqence Model Course</b> at <a href=\"https://www.coursera.org/learn/nlp-sequence-models\"> Coursera</a> </li>\n    \n<li> <b>Andrej Karpathy's Blog</b> on <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">Effectiveness of RNN</a></li>\n\n<li> <b>Intuitive Understanding of GloVe Embedding</b> on <a href=\"https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\">TDS</a></li>\n\n<li> <b>Keras tutorial on Word Embedding</b> <a href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\"> here</a></li>\n\n</ul>\n</font>","metadata":{}},{"cell_type":"markdown","source":"> <font color='#696969'>I got to say like you, I am still at learning phase in terms of NLP. I have got lot to learn in future. I found that writing this notebook even though it is done by lot of people before me helps me with a deeper and complete understanding our the concepts that I am learning. Kaggle has been a amazing place to learn from and contribute to community of Data Science Aspirants.</font>","metadata":{}},{"cell_type":"markdown","source":"<h2><font color='red'> If you find this notebook usefull kindly UPVOTE this notebook. I am new to writting notebooks hope that would really encourage me to write and learn more.</font></h2>\n\n<h5>Thanks in Advance. Have a nice day. Learn more and Happy Kaggle</h5>","metadata":{}}]}